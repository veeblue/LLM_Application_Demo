import os
from pathlib import Path
import time
from typing import List

import streamlit as st

from llama_index.core import VectorStoreIndex
from llama_index.core.chat_engine.utils import get_response_synthesizer
import gradio as gr
from llama_index.core.schema import TextNode

from config import Config
from legal_assistant import load_and_validate_json_files, create_nodes, init_models, init_vector_store

# ================== Streamlité¡µé¢é…ç½® ==================
st.set_page_config(
    page_title="æ™ºèƒ½åŠ³åŠ¨æ³•å’¨è¯¢åŠ©æ‰‹",
    page_icon="âš–ï¸",
    layout="centered",
    initial_sidebar_state="auto"
)

# ================== ç¦ç”¨Streamlitæ–‡ä»¶ç›‘æ§ ==================
# def disable_streamlit_watcher():
#     """Patch Streamlit to disable file watcher"""
#
#     def _on_script_changed(_):
#         return
#
#     from streamlit import runtime
#     runtime.get_instance()._on_script_changed = _on_script_changed

@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½æ¨¡å‹...")
def st_init_models():
    """åˆå§‹åŒ–æ¨¡å‹"""
    embed_model, llm, reranker = init_models()
    return embed_model, llm, reranker

@st.cache_resource(show_spinner="æ­£åœ¨åŠ è½½æ•°æ®...")
def st_init_vector_store(_nodes):
    return init_vector_store(_nodes)

def init_chat_interface():
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        role = msg["role"]
        content = msg.get("cleaned", msg["content"])  # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹

        with st.chat_message(role):
            st.markdown(content)

            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å«æ€ç»´é“¾
            if role == "assistant" and msg.get("think"):
                with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆå†å²å¯¹è¯ï¼‰"):
                    for think_content in msg["think"]:
                        st.markdown(f'<span style="color: #808080">{think_content.strip()}</span>',
                                    unsafe_allow_html=True)

            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”æœ‰å‚è€ƒä¾æ®ï¼ˆéœ€è¦ä¿æŒåŸæœ‰å‚è€ƒä¾æ®é€»è¾‘ï¼‰
            if role == "assistant" and "reference_nodes" in msg:
                show_reference_details(msg["reference_nodes"])


def show_reference_details(nodes):
    with st.expander("æŸ¥çœ‹æ”¯æŒä¾æ®"):
        for idx, node in enumerate(nodes, 1):
            meta = node.node.metadata
            st.markdown(f"**[{idx}] {meta['full_title']}**")
            st.caption(f"æ¥æºæ–‡ä»¶ï¼š{meta['source_file']} | æ³•å¾‹åç§°ï¼š{meta['law_name']}")
            st.markdown(f"ç›¸å…³åº¦ï¼š`{node.score:.4f}`")
            # st.info(f"{node.node.text[:300]}...")
            st.info(f"{node.node.text}")

import re
def main():
    # ç¦ç”¨ Streamlit æ–‡ä»¶çƒ­é‡è½½
    # disable_streamlit_watcher()
    st.title("âš–ï¸ æ™ºèƒ½åŠ³åŠ¨æ³•å’¨è¯¢åŠ©æ‰‹")
    st.markdown("æ¬¢è¿ä½¿ç”¨åŠ³åŠ¨æ³•æ™ºèƒ½å’¨è¯¢ç³»ç»Ÿï¼Œè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†åŸºäºæœ€æ–°åŠ³åŠ¨æ³•å¾‹æ³•è§„ä¸ºæ‚¨è§£ç­”ã€‚")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "history" not in st.session_state:
        st.session_state.history = []

    # åŠ è½½æ¨¡å‹å’Œç´¢å¼•
    embed_model, llm, reranker = st_init_models()

    # åˆå§‹åŒ–æ•°æ®
    if not Path(Config.VECTOR_DB_DIR).exists():
        with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“..."):
            raw_data = load_and_validate_json_files(Config.DATA_DIR)
            nodes = create_nodes(raw_data)
    else:
        nodes = None

    index = st_init_vector_store(nodes)
    retriever = index.as_retriever(similarity_top_k=Config.TOP_K, vector_store_query_mode="hybrid", alpha=0.5)
    from llama_index.core.response_synthesizers import TreeSummarize
    from llama_index.llms.huggingface import HuggingFaceLLM
    response_synthesizer = TreeSummarize(llm=embed_model)
    # èŠå¤©ç•Œé¢
    init_chat_interface()

    if prompt := st.chat_input("è¯·è¾“å…¥åŠ³åŠ¨æ³•ç›¸å…³é—®é¢˜"):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # å¤„ç†æŸ¥è¯¢
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜..."):
            start_time = time.time()

            # æ£€ç´¢æµç¨‹
            initial_nodes = retriever.retrieve(prompt)
            reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=prompt)

            # è¿‡æ»¤èŠ‚ç‚¹
            MIN_RERANK_SCORE = 0.4
            filtered_nodes = [node for node in reranked_nodes if node.score > MIN_RERANK_SCORE]

            if not filtered_nodes:
                response_text = "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼Œè¯·å°è¯•è°ƒæ•´é—®é¢˜æè¿°æˆ–å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆã€‚"
            else:
                # ç”Ÿæˆå›ç­”
                response = response_synthesizer.synthesize(prompt, nodes=filtered_nodes)
                response_text = response.response

            # æ˜¾ç¤ºå›ç­”
            with st.chat_message("assistant"):
                # æå–æ€ç»´é“¾å†…å®¹å¹¶æ¸…ç†å“åº”æ–‡æœ¬
                think_contents = re.findall(r'<think>(.*?)</think>', response_text, re.DOTALL)
                cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()

                # æ˜¾ç¤ºæ¸…ç†åçš„å›ç­”
                st.markdown(cleaned_response)

                # å¦‚æœæœ‰æ€ç»´é“¾å†…å®¹åˆ™æ˜¾ç¤º
                if think_contents:
                    with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                        for content in think_contents:
                            st.markdown(f'<span style="color: #808080">{content.strip()}</span>',
                                        unsafe_allow_html=True)

                # æ˜¾ç¤ºå‚è€ƒä¾æ®ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                show_reference_details(filtered_nodes[:3])

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²ï¼ˆéœ€è¦å­˜å‚¨åŸå§‹å“åº”ï¼‰
            st.session_state.messages.append({
                "role": "assistant",
                "content": response_text,  # ä¿ç•™åŸå§‹å“åº”
                "cleaned": cleaned_response,  # å­˜å‚¨æ¸…ç†åçš„æ–‡æœ¬
                "think": think_contents  # å­˜å‚¨æ€ç»´é“¾å†…å®¹
            })

if __name__ == "__main__":
    main()